{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\" # suppress info and warning messages\n",
        "import tensorflow.keras as keras\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "_tD1LpdExKoP",
        "outputId": "219b32c2-65df-49db-d025-c5983e50cd8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# since we use our own dataset, our importing process is a bit different. We downloaded our data set and ran the code cells on our local Jupyter Notebook\n",
        "dataset_dir = '/Users/sophiacherkaoui/Downloads/living_room_dataset'"
      ],
      "metadata": {
        "id": "UYfEvphD1Pa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr5bBg9uw2aT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "9c25b41e-4a82-41e3-b5df-8d7f679f0785"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Could not find directory /Users/sophiacherkaoui/Downloads/living_room_dataset",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-546088682.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrentals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dataset_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inferred'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Deluxe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Luxury\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Standard\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrentals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdataset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory /Users/sophiacherkaoui/Downloads/living_room_dataset"
          ]
        }
      ],
      "source": [
        "# load in the dataset from a folder \"living_room_dataset\", where each subdirectory is named [label] and contains a subset of images with that label\n",
        "rentals = keras.utils.image_dataset_from_directory(dataset_dir, labels='inferred', class_names=[\"Deluxe\", \"Luxury\", \"Standard\"], seed=1234)\n",
        "dataset_size = len(rentals)\n",
        "\n",
        "# split the data into 90% training, 10% test sets\n",
        "train_size = int(0.9 * dataset_size)\n",
        "train_ds = rentals.take(train_size)\n",
        "test_ds = rentals.skip(train_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prints four random examples from dataset\n",
        "for i in range(1, 5):\n",
        "    img_tensor, label = next(iter(rentals.unbatch()))\n",
        "    img_np = img_tensor.numpy()\n",
        "    plt.imshow(img_np.astype(\"uint8\"))\n",
        "    plt.title(rentals.class_names[label])\n",
        "    plt.axis(False)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9M172KwoOKKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print shape of one example\n",
        "img_np.shape"
      ],
      "metadata": {
        "id": "rNfX5wnHtnAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert training and test datasets to numpy arrays. each item in train_ds, test_ds is a tuple in the form (image_batch, label_batch)\n",
        "def to_numpy(dataset):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for image_batch, label_batch in dataset:\n",
        "        # convert examples to numpy arrays and add them to lists\n",
        "        images.append(image_batch.numpy())\n",
        "        labels.append(label_batch.numpy())\n",
        "\n",
        "    # merge all examples and labels into their respective sets\n",
        "    X = np.concatenate(images, axis=0)\n",
        "    y = np.concatenate(labels, axis=0)\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "JvMiO09otv5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call function on training and test batches\n",
        "\n",
        "X_train, y_train = to_numpy(train_ds)\n",
        "X_test, y_test = to_numpy(test_ds)"
      ],
      "metadata": {
        "id": "b0Bjrfmytyy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize pixel values\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# verify values were normalized (all values should be floats in range [0.0, 1.0])\n",
        "X_train[0]\n",
        "X_test[0]"
      ],
      "metadata": {
        "id": "2xfhXU56t1bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect training and test sets\n",
        "print(\"X_train: \" + str(X_train.shape))\n",
        "print(\"X_test: \" + str(X_test.shape))\n",
        "print(\"y_train: \" + str(y_train.shape))\n",
        "print(\"y_test: \" + str(y_test.shape))"
      ],
      "metadata": {
        "id": "XWUIMRZVt4Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify shape of one example\n",
        "# correct format: (pixel_height, pixel_width, num_color_channels)\n",
        "# color channels = 3 since the images are RGB\n",
        "X_train[0].shape"
      ],
      "metadata": {
        "id": "rHFaVGTmt8Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building the model\n",
        "cnn_model = keras.Sequential()\n",
        "\n",
        "input_shape = X_train[0].shape\n",
        "input_layer = keras.layers.InputLayer(input_shape)\n",
        "cnn_model.add(input_layer)\n",
        "\n",
        "# create first hidden layer\n",
        "conv_1 = keras.layers.Conv2D(16, 3)\n",
        "batchNorm_1 = keras.layers.BatchNormalization()\n",
        "ReLU_1 = keras.layers.ReLU()\n",
        "cnn_model.add(conv_1)\n",
        "cnn_model.add(batchNorm_1)\n",
        "cnn_model.add(ReLU_1)\n",
        "\n",
        "# create second hidden layer\n",
        "conv_2 = keras.layers.Conv2D(32, 3)\n",
        "batchNorm_2 = keras.layers.BatchNormalization()\n",
        "ReLU_2 = keras.layers.ReLU()\n",
        "cnn_model.add(conv_2)\n",
        "cnn_model.add(batchNorm_2)\n",
        "cnn_model.add(ReLU_2)\n",
        "\n",
        "# add pooling layer\n",
        "pooling_layer = keras.layers.GlobalAveragePooling2D()\n",
        "cnn_model.add(pooling_layer)\n",
        "\n",
        "output_layer = keras.layers.Dense(units=5)\n",
        "cnn_model.add(output_layer)\n",
        "\n",
        "cnn_model.summary()"
      ],
      "metadata": {
        "id": "iok-knaNt-gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify a stochastic gradient descent optimizer with an appropriate learning rate\n",
        "sgd_optimizer = keras.optimizers.SGD(learning_rate=0.1)"
      ],
      "metadata": {
        "id": "EMmANHpJuBu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify the loss function\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "brzKEJoHuFN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "cnn_model.compile(optimizer=sgd_optimizer, loss=loss_fn, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "eMEnE48euHzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "num_epochs = 1\n",
        "t0 = time.time() # start time\n",
        "history = cnn_model.fit(X_train, y_train, epochs=num_epochs)\n",
        "t1 = time.time() # stop time\n",
        "\n",
        "print('Elapsed time: %.2fs' % (t1-t0))"
      ],
      "metadata": {
        "id": "Bk1-pvViuKWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate and print the model's performance\n",
        "loss, accuracy = cnn_model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Loss: ', str(loss) , 'Accuracy: ', str(accuracy))"
      ],
      "metadata": {
        "id": "xDrOqntFuP-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Defining ML Problem\n",
        "\n",
        "Questions:\n",
        "1. List the data set you have chosen.\n",
        "2. What will you be predicting? What is the label?\n",
        "3. Is this a supervised or unsupervised learning problem? Is this a clustering, classification or regression problem? Is it a binary classificaiton or multi-class classifiction problem?\n",
        "4. What are your features? (note: this list may change after your explore your data)\n",
        "5. Explain why this is an important problem. In other words, how would a company create value with a model that predicts this label?\n",
        "\n",
        "## The Answers\n",
        "\n",
        "1. The data set we chose is the airbnbListings data set. However, due to the nature of our project, we decided to do augment this data set with images scraped from the Airbnb website. We utilized scrapers from the following repositories to obtain images and prices:\n",
        "\n",
        "https://github.com/johnbalvin/pyairbnb\n",
        "\n",
        "https://github.com/airbert-vln/bnb-dataset/tree/main\n",
        "\n",
        "However, we made some edits to these scripts when necessary to fit our desired dataset size. We also created some of our own scripts to further assist in the data organization. These can be found on our [Github](https://github.com/soph743/cnn_pricing_model).\n",
        "\n",
        "2. We are predicting the price range, and therefore tier (standard, deluxe or luxury) of Airbnb listings depending on images of the listing's bedroom.\n",
        "\n",
        "3. This is a supervised learning problem because we predefined our labels as standard, deluxe or luxury. The nightly pricing thresholds are as followed:\n",
        "\n",
        "- Standard: $137 or less\n",
        "\n",
        "- Deluxe: $138 - $240\n",
        "\n",
        "- Luxury: $241+\n",
        "\n",
        "This problem is also a multi-classification problem because we grouped each bedroom image into one of the three categories above.\n",
        "\n",
        "4. Since this is an image classication problem, our features are the actual images which the model was trained on. As the Convoluted Neural Network learns on these images, it uses certain patterns it detects in pixel value to classify the test data.\n",
        "\n",
        "5. This is an important problem because Airbnb has over 8 million listings worldwide, humans cannot possibly sift through all of these listings in a timely manner, and also provide quality service for helping guests find their next stay. With an automated system like this, we greatly increase the efficiency of searching and filtering for hosts and customers. This is also an important problem because it improves the service of hosts and therefore customer satisfaction. For instance, this model could help new hosts improve their listings with visual feedback, and we could even add a feature where we provide a checklist for hosts to address. For example, “To be considered ‘Deluxe’, consider adding X, Y, Z.” A company could create value with a model that predicts this label tier for several reasons. One, auto-mated quality assessment. For instance, instead of relying solely on host-provided descriptions (which are often fabricated for marketing purposes we researched), Airbnb could automatically assess the visual quality of a listing's interior. Airbnb could even go one step further to suggest approrpiate pricing tiers, flag any listings that don’t match their claimed category and therefore improve consistency and trust with guests/consumers on the platform. Moreover, in terms of scaling our project, we could plan potential integration into recommendation engines; Such as filtering listings by visual appeal & tier without requiring structured input data (maybe expand to Zillow, Redfin, etc.). Also, we could provide property managers/hosts with visual feedback to guide renovation or staging decisions aligned with target pricing tiers. Our model can also be integrated with a front-end application where users can attach images of their rental properties, and recieve a suggested price range. Such an application can be utilized by Airbnb or other vacation rental sites to help hosts determine appropriate pricing for their properties."
      ],
      "metadata": {
        "id": "h3GELu3EgemF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Define Your Project Plan\n",
        "\n",
        "\n",
        "\n",
        "1.  Do you have a new feature list? If so, what are the features that you chose to keep and remove after inspecting the data?\n",
        "2. Explain different data preparation techniques that you will use to prepare your data for modeling.\n",
        "3. What is your model (or models)?\n",
        "4. Describe your plan to train your model, analyze its performance and then improve the model. That is, describe your model building, validation and selection plan to produce a model that generalizes well to new data.\n",
        "\n",
        "## The Answers\n",
        "\n",
        "1. Yes, we chose to omit all features in the Airbnb listing dataset, as stated previously. Our images act as the features which our CNN is trained on.\n",
        "2. The different data preparation techniques we used to prepare our data for modeling included:\n",
        "- Running scripts to extract, organize, and resize our images.\n",
        "- Scraping information on nightly price for each listing, which we then used to determine the ranges for our three classes: Standard, Deluxe and Luxury.\n",
        "- Choosing a subset of images with similar characteristics to allow for better model predictions (in our case, we chose only bedroom images).\n",
        "- Normalizing pixel data in Jupyter Notebook.\n",
        "- Addressing class imbalance in our data sample to promote fair AI in a couple of ways. 1, by making the price cut offs in a way that puts a close amount of images in each category (standard, deluxe or luxury). 2, we didn't try to arrange the images by our own standards, rather the images were selected randomly using an automated scraper tool.\n",
        "3. Our model is a convolutional neural network (CNN) that classifies Airbnb properties into 3 price tiers (standard, deluxe, luxury), based solely on 85 uploaded images that extract visual features. We used a small dataset to speed up the training process; our intention is to scale up the model with larger sets of images in order to improve model accuracy.\n",
        "4. Due to the small size of data used, paired with the complexity of our problem (predicting price of a rental property based off of bedroom photos), we do not expect a high accuracy. When we initially trained the model, we had an accuracy of 57% and a loss of 7. This high loss shows the model's inability to learn patterns in the data adequately, which is what we predicted due to the reasons discussed previously.\n",
        "\n",
        "However, we were able to minimize the loss by implementing a few changes:\n",
        "- including an input layer, 2 hidden layers, a pooling layer and output layer: due to the small dataset used, extra layers are not necessary and can lead to overfitting.\n",
        "- setting the learning rate to 0.1: we found that decreasing the learning rate increased the loss slightly and did not improve accuracy.\n",
        "- setting the filter size in the hidden convolution layers to (16, 3) and (32, 3): we found that altering the filter sizes to be higher/lower in each layer led to a decrease in accuracy.\n",
        "\n",
        "Our changes resulted in a model with 57% accuracy and loss of 1.3. These statistics show that our model is still poorly suited to generalization on new data, as the CNN has not properly learning to classify the images. Below are some steps we can take to improve model accuracy:\n",
        "\n",
        "1. Increase the size of the data set.\n",
        "2. Select one city/zip code to pull data from: since we used a set of 12 cities in the US, this may have added extra complexity to the model. By sticking to one city, the model may be able to learn pricing patterns better.\n",
        "3. Add non-image features: extra features such as square footage may provide useful additional information to improve predictions.\n",
        "4. Address class imbalance: our \"luxury\" category had the least amount of examples, as it was the category with the highest prices. We can add more images in this category.\n",
        "5. Use data augmentation: by changing the orientation of the images, we can prevent overfitting.\n",
        "6. Implement more techniques learned in class such as K-fold cross-validation, L2 regularization and dropout.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b_TH4Bw9hQ7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our main motive behind choosing this project is was to challenge ourselves. It is beyond the scope of the requirements of this project, yet we followed our ambition and intellectual curiousity to try something different by building a convolutional neural network. We have a vision to continue to this work and even implement a front-end part. We want to incorporate a lightweight Flask interface for an accessible, browser-based experience which grants a clean, intuitive output for non-technical users."
      ],
      "metadata": {
        "id": "_FqcGmAz3v5U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dNA8Kk3_3sL0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}